---
layout: post
title:  "ESVIO代码详解"
info: "阅读ESVIO代码并解释核心功能"
tech: " 持续更新ing "
type: Brief introduction 
---

#### 整体逻辑: 

<table rules="none" align="center">
	<tr>
		<td>
			<center>
				<img src="https://effun.xyz/assets/img/20250708/rosgraph_esvio.png" width="100%" />
				<br/>
				<font color="AAAAAA"></font>
			</center>
		</td>
	</tr>
</table>

代码底层框架源于vins，包含三个独立的ros功能包：feature_tracker, esvio_estimator和pose_graph。

## &#127774;功能包1：feature_tacker

**包含event_detecor、feature_tracker、parameter、stereo_event_tracker_node、stereo_image_tracker_node五个核心文件。**

包含两个节点：stereo_image_tracker和stereo_event_tracker

### &#9889;1. stereo_image_tracker_node的main函数

从main函数分析关键函数的调用链：

```
int main(int argc, char **argv)
{
    ros::init(argc, argv, "stereo_image_tracker");
    ros::NodeHandle n("~");
    ros::console::set_logger_level(ROSCONSOLE_DEFAULT_NAME, ros::console::levels::Info);
    readParameters(n);

    // 初始化相机参数
    trackerData.stereo_readIntrinsicParameter(CAM_NAMES);

    // 订阅左右相机图像
    ros::Subscriber sub_img_left = n.subscribe(IMAGE_LEFT, 100, img_callback_left);
    ros::Subscriber sub_img_right = n.subscribe(IMAGE_RIGHT, 100, img_callback_right);

    // 发布特征点和可视化信息
    pub_img = n.advertise<sensor_msgs::PointCloud>("feature", 1000);
    pub_match = n.advertise<sensor_msgs::Image>("feature_img",1000);
    pub_restart = n.advertise<std_msgs::Bool>("restart",1000);

    // 启动同步线程
    std::thread sync_thread{sync_process};
    ros::spin();
    return 0;
}

```

关键函数调用链：

* readParameters(n) → 读取配置文件 → 初始化全局参数

* trackerData.stereo_readIntrinsicParameter(CAM_NAMES) → 读取相机内参

* std::thread sync_thread{sync_process} → 启动同步线程

* sync_process() → 同步左右相机图像

* getImageFromMsg() → 转换ROS图像消息为OpenCV格式

* handle_stereo_image() → 处理立体图像

* trackerData.trackImage() → 特征跟踪核心函数


### &#9889;2. stereo_event_tracker_node的main函数

基本同上

### &#9889;3. sync_process()事件相机函数

值得学习的是对事件的处理方式

首先说明，对于事件队列：

`这是左右相机事件的定义：queue<dvs_msgs::EventArray> events_left_buf;`


队列（queue）：这是C++标准库中的一个容器适配器，实现了FIFO（先进先出）的数据结构。主要操作有：

    push：添加元素到队尾
    pop：移除队首元素
    front：访问队首元素
    empty：检查队列是否为空




dvs_msgs::EventArray：这是一个ROS消息类型，用于存储事件相机捕获的事件数据。从你提供的头文件可以看出它包含以下字段：

    header：标准ROS消息头，包含时间戳和坐标帧ID
    height：图像高度（行数）
    width：图像宽度（列数）
    events：事件数组，类型为Event[]



Event 结构：根据定义，每个Event包含：

    x：事件在图像上的x坐标（列）
    y：事件在图像上的y坐标（行）
    ts：事件的时间戳
    polarity：事件的极性（true或false，表示亮度增加或减少）

所以，queue<dvs_msgs::EventArray> events_left_buf 是一个队列，用于存储来自左相机的事件数据包。每个数据包（EventArray）包含在特定时间窗口内捕获的一系列事件。

在sync_process函数中，每次循环只处理队列中的一个dvs_msgs::EventArray消息

```
void sync_process()
{
    while(1)
    {
            dvs_msgs::EventArray event_left, event_right;
            std_msgs::Header header;
            double msg_timestamp = 0.0;

            m_buf_event.lock(); // 锁定事件缓冲区互斥量，防止在访问缓冲区时发生竞争条件

            // 检查左右事件缓冲区是否都不为空
            if (!events_left_buf.empty() && !events_right_buf.empty()){
                // 获取左右事件队列前端的时间戳
                double time_left = events_left_buf.front().header.stamp.toSec();
                double time_right = events_right_buf.front().header.stamp.toSec();
                // 使用左相机事件的时间戳作为消息时间戳
                msg_timestamp = time_left;

                // 如果左相机事件时间戳比右相机早0.2秒以上，则丢弃左相机事件
                if(time_left < time_right - 0.2) //tolerance
                {
                    events_left_buf.pop();
                    printf("throw events1\n");
                }
                // 如果左相机事件时间戳比右相机晚0.2秒以上，则丢弃右相机事件
                else if(time_left > time_right + 0.2)
                {
                    events_right_buf.pop();
                    printf("throw events2\n");
                }
                // 如果两个事件的时间戳差距在0.2秒内，则认为它们是同步的
                else
                {
                    // 记录消息时间戳和头部信息
                    msg_timestamp = events_left_buf.front().header.stamp.toSec();
                    header = events_left_buf.front().header;
                    // 获取左相机事件
                    event_left = events_left_buf.front();
                    // 移除处理过的左相机事件
                    events_left_buf.pop();
                    // 获取右相机事件
                    event_right = events_right_buf.front();
                    // 移除处理过的右相机事件
                    events_right_buf.pop();
                }

            }
            // 解锁互斥量
            m_buf_event.unlock();

            if(event_left.events.size()!= 0){

                handle_stereo_event(event_left, event_right, msg_timestamp);

            }

        std::chrono::milliseconds dura(2);
        std::this_thread::sleep_for(dura);
    }
    

}
```


### &#9889;4. handle_stereo_event函数

在trackerData.trackEvent()函数中，完成左右相机特征点的检测和匹配，匹配的结果保存在trackerData对象中，包括：

    trackerData.ids: 左相机特征点的ID
    trackerData.ids_right: 右相机特征点的ID
    以及相应的位置、速度等信息

当同一个特征点在左右相机中都被检测到时，它们被赋予相同的特征ID，在`hash_ids`集合中保存了左相机所有特征点的ID，通过检查右相机特征点的ID是否在hash_ids中存在，可以确定这个特征点是否在两个相机中都被检测到

具体实现逻辑

    特征跟踪和匹配：
        在trackerData.trackEvent()函数中，对左右相机的事件数据进行处理
        检测特征点并进行双目匹配（基于特征描述子、空间位置或极线约束等）
        成功匹配的特征点被赋予相同的ID

    ID分配机制：
        新检测到的特征点获得新的唯一ID
        匹配成功的左右相机特征点共享同一个ID
        这些ID被分别存储在trackerData.ids和trackerData.ids_right中

    发布前的过滤：
        代码首先处理左相机的特征点，并将其ID存入hash_ids
        然后处理右相机的特征点，但只保留那些ID在hash_ids中存在的点
        这确保了只发布双目匹配成功的特征点对


```
void handle_stereo_event(const dvs_msgs::EventArray &event_left, const dvs_msgs::EventArray &event_right, double msg_timestamp)
{   
    static int cnt = 0;
    const int n_event = event_left.events.size();

    if(n_event == 0){
        ROS_WARN("not event, please move the event camera or check whether connecting");
        return;
    }

    if(first_image_flag)// 处理第一帧事件的特殊情况
    {
        first_image_flag = false;
        first_image_time = msg_timestamp;
        last_image_time = msg_timestamp;
        return;
    }

    // 检测事件流是否连续，如果时间戳异常则重置跟踪器
    if (msg_timestamp - last_image_time > 1.0 || msg_timestamp < last_image_time)
    {
        ROS_WARN("event stream discontinue! reset the event feature tracker!");
        first_image_flag = true; 
        last_image_time = 0;
        pub_count = 1;
        std_msgs::Bool restart_flag;
        restart_flag.data = true;
        pub_restart.publish(restart_flag);// reset para and reset operation
        return;
    }
    last_image_time = msg_timestamp;

    // frequency control
    if (round(1.0 * pub_count / (msg_timestamp - first_image_time)) <= FREQ) // ?这里的msg_timestamp 还有first_image_time的赋值逻辑
    {
        PUB_THIS_FRAME = true;// 标记当前帧需要发布

        // 当达到目标频率时重置计时
        if (abs(1.0 * pub_count / (msg_timestamp - first_image_time) - FREQ) < 0.01 * FREQ)
        {
            first_image_time = msg_timestamp;
            pub_count = 0;
        }
    }
    else
        PUB_THIS_FRAME = false;

    double msg_timestamp_left = event_left.events.back().ts.toSec();

    if (Do_motion_correction == 0){
        // 不进行运动校正，直接跟踪事件
        trackerData.trackEvent(msg_timestamp_left, event_left, event_right);//optiflow track 核心功能4 匹配
    }else{
        Motion_correction_value motion_compensation;// 进行运动校正，需要计算运动补偿参数
        // 获取事件批次的起始和结束时间
        double t_left_0 = event_left.events[0].ts.toSec();  // 第一个事件的时间戳
        double t_left_1 = event_left.header.stamp.toSec();  // 事件批次的时间戳
        
        // 声明用于存储动态信息的变量
        Eigen::Vector3d temp_v;  // 当前速度
        Eigen::Vector3f temp_a;  // 当前加速度
        double temp_time;        // 当前时间
        Eigen::Vector4d State_;  // 保存当前状态和时间
        Eigen::Vector2d t_0_1(t_left_0,t_left_1);  // 事件时间窗口
        Eigen::Vector3f omega_avg_;  // IMU角速度
        Eigen::Vector3f accel_avg_;  // IMU加速度

        // 如果有IMU数据
        if (!imu_buf.empty()){
            // 如果有里程计数据，获取当前速度
            if(!odom_buffer_.empty()){
                temp_time = odom_buffer_.front()->header.stamp.toSec();
                // 获取当前无人机速度
                temp_v[0]=odom_buffer_.front()->twist.twist.linear.x;
                temp_v[1]=odom_buffer_.front()->twist.twist.linear.y;
                temp_v[2]=odom_buffer_.front()->twist.twist.linear.z;
                odom_buffer_.pop();  // 弹出处理过的里程计数据

                // 保存当前状态
                State_[0]=temp_v[0];
                State_[1]=temp_v[1];
                State_[2]=temp_v[2];
                State_[3]=temp_time;

                // 更新前一时刻和当前时刻的速度
                v_pre[0] = v_cur[0];
                v_pre[1] = v_cur[1];
                v_pre[2] = v_cur[2];
                v_cur[0] = temp_v[0];
                v_cur[1] = temp_v[1];
                v_cur[2] = temp_v[2];

                // 更新前一时刻和当前时刻的时间
                t_pre = t_cur;  //t_pre存储上一次获取里程计数据的时间戳
                t_cur = temp_time; //t_cur存储当前获取的里程计数据的时间戳

                // 计算加速度（速度差除以时间差）
                temp_a[0] = (v_cur[0] - v_pre[0])/(t_cur - t_pre);  //_cur - t_pre是连续两次获取里程计数据之间的时间差
                temp_a[1] = (v_cur[1] - v_pre[1])/(t_cur - t_pre);
                temp_a[2] = (v_cur[2] - v_pre[2])/(t_cur - t_pre);
            }

            // 移除事件时间窗口之前的IMU数据
            while(!imu_buf.empty()){
                if (imu_buf.front()->header.stamp.toSec() < t_left_0)
                    imu_buf.pop();
                else
                    break;
            }

            // 获取IMU数据（角速度和线性加速度）
            if (!imu_buf.empty()){
                omega_avg_[0]=imu_buf.front()->angular_velocity.x;
                omega_avg_[1]=imu_buf.front()->angular_velocity.y;
                omega_avg_[2]=imu_buf.front()->angular_velocity.z;
                accel_avg_[0]=imu_buf.front()->linear_acceleration.x;
                accel_avg_[1]=imu_buf.front()->linear_acceleration.y;
                accel_avg_[2]=imu_buf.front()->linear_acceleration.z-9.805;  // 减去重力加速度
            }
        }
        
        // 构建运动补偿数据结构 ？实现方法
        motion_compensation = std::make_pair(is_nolinear,std::make_pair(std::make_pair (State_,v_pre),std::make_pair(t_0_1,std::make_pair(temp_a,omega_avg_))));

        // 使用运动补偿参数跟踪事件
        trackerData.trackEvent(msg_timestamp_left, event_left, event_right, motion_compensation);
    }

    if (SHOW_TRACK)//show the tracking process
        {
            cv::Mat imageTrack=trackerData.getTrackImage();
            cv::Mat imgTrack_two =trackerData.getTrackImage_two();
            cv::Mat imgTrack_two_point =trackerData.getTrackImage_two_point();
            cv::Mat Time_surface_map =trackerData.gettimesurface();
            cv::Mat event_loop = trackerData.getEventloop();
            pubTrackImage(imageTrack,imgTrack_two,imgTrack_two_point,Time_surface_map,last_image_time,event_loop);
        }


   // 如果需要发布当前帧的特征点 创建和填充了一个ROS点云消息，用于传输特征点信息
    if (PUB_THIS_FRAME)
    {
        pub_count++;  // 更新发布计数

        {
            // 创建点云消息，用于存储特征点 智能指针，指向新分配的PointCloud消息对象 PointCloud是ROS中传输3D点集合的标准消息类型
            sensor_msgs::PointCloudPtr feature_points(new sensor_msgs::PointCloud);
            // 创建特征点的各种属性通道
            sensor_msgs::ChannelFloat32 id_of_point;         // 特征点ID
            sensor_msgs::ChannelFloat32 u_of_point;          // 特征点u坐标
            sensor_msgs::ChannelFloat32 v_of_point;          // 特征点v坐标
            sensor_msgs::ChannelFloat32 velocity_x_of_point; // 特征点x方向速度
            sensor_msgs::ChannelFloat32 velocity_y_of_point; // 特征点y方向速度

            // 设置点云消息的时间戳和坐标系
            feature_points->header.stamp = ros::Time(msg_timestamp);
            feature_points->header.frame_id = "world";

            int camera_id = 0;  // 左相机ID为0
            int feature_id;
            geometry_msgs::Point32 p;
            
            // 处理左图像上的特征点
            set<int> hash_ids;  // 用于跟踪已处理的特征点ID
            for(size_t j=0; j<trackerData.ids.size(); j++){
               // 只处理被跟踪超过1帧的特征点
               if (trackerData.track_cnt[j] > 1)
                {
                    feature_id = trackerData.ids[j];
                    // 归一化平面上的坐标
                    p.x = trackerData.cur_un_pts[j].x;
                    p.y = trackerData.cur_un_pts[j].y;
                    p.z = 1;
                    
                    // 记录已处理的特征点ID
                    hash_ids.insert(feature_id);
                    // 添加特征点到点云
                    feature_points->points.push_back(p);
                    // 添加特征点属性
                    id_of_point.values.push_back(feature_id * NUM_OF_CAM_stereo + camera_id);
                    u_of_point.values.push_back(trackerData.cur_pts[j].x);
                    v_of_point.values.push_back(trackerData.cur_pts[j].y);
                    velocity_x_of_point.values.push_back(trackerData.pts_velocity[j].x);
                    velocity_y_of_point.values.push_back(trackerData.pts_velocity[j].y);
                } 
            }
            
            // 处理右图像上的特征点
            camera_id = 1;  // 右相机ID为1
            for(size_t j=0; j<trackerData.ids_right.size(); j++){
               feature_id = trackerData.ids_right[j];
               // 只处理在左图像中也被跟踪的特征点（双目匹配点）trackEvent函数已经完成了匹配
               if (hash_ids.find(feature_id) != hash_ids.end())
                {
                    // 归一化平面上的坐标
                    p.x = trackerData.cur_un_right_pts[j].x;
                    p.y = trackerData.cur_un_right_pts[j].y;
                    p.z = 1;
                    // 添加特征点到点云
                    feature_points->points.push_back(p);
                    // 添加特征点属性
                    id_of_point.values.push_back(feature_id * NUM_OF_CAM_stereo + camera_id);
                    u_of_point.values.push_back(trackerData.cur_right_pts[j].x);
                    v_of_point.values.push_back(trackerData.cur_right_pts[j].y);
                    velocity_x_of_point.values.push_back(trackerData.right_pts_velocity[j].x);
                    velocity_y_of_point.values.push_back(trackerData.right_pts_velocity[j].y);
                } 
            }
            
            // 将所有属性通道添加到点云消息
            feature_points->channels.push_back(id_of_point);
            feature_points->channels.push_back(u_of_point);
            feature_points->channels.push_back(v_of_point);
            feature_points->channels.push_back(velocity_x_of_point);
            feature_points->channels.push_back(velocity_y_of_point);

            // 跳过第一帧（因为第一帧没有光流速度）
            if (!init_pub)
            {
                init_pub = 1;
            }
            else
                // 发布特征点消息到姿态图优化节点
                pub_img.publish(feature_points);
        }
    }
}
```
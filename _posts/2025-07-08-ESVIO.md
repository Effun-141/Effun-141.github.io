---
layout: post
title:  "ESVIO代码详解"
info: "阅读ESVIO代码并解释核心功能"
tech: " 持续更新ing "
type: Brief introduction 
---

#### 整体逻辑: 

<table rules="none" align="center">
	<tr>
		<td>
			<center>
				<img src="https://effun.xyz/assets/img/20250708/rosgraph_esvio.png" width="100%" />
				<br/>
				<font color="AAAAAA"></font>
			</center>
		</td>
	</tr>
</table>

代码底层框架源于vins，包含三个独立的ros功能包：feature_tracker, esvio_estimator和pose_graph。

## &#127774;功能包1：feature_tacker

**包含event_detecor、feature_tracker、parameter、stereo_event_tracker_node、stereo_image_tracker_node五个核心文件。**

包含两个节点：stereo_image_tracker和stereo_event_tracker

### &#9889;1. stereo_image_tracker_node的main函数

从main函数分析关键函数的调用链：

```
int main(int argc, char **argv)
{
    ros::init(argc, argv, "stereo_image_tracker");
    ros::NodeHandle n("~");
    ros::console::set_logger_level(ROSCONSOLE_DEFAULT_NAME, ros::console::levels::Info);
    readParameters(n);

    // 初始化相机参数
    trackerData.stereo_readIntrinsicParameter(CAM_NAMES);

    // 订阅左右相机图像
    ros::Subscriber sub_img_left = n.subscribe(IMAGE_LEFT, 100, img_callback_left);
    ros::Subscriber sub_img_right = n.subscribe(IMAGE_RIGHT, 100, img_callback_right);

    // 发布特征点和可视化信息
    pub_img = n.advertise<sensor_msgs::PointCloud>("feature", 1000);
    pub_match = n.advertise<sensor_msgs::Image>("feature_img",1000);
    pub_restart = n.advertise<std_msgs::Bool>("restart",1000);

    // 启动同步线程
    std::thread sync_thread{sync_process};
    ros::spin();
    return 0;
}

```

关键函数调用链：

* readParameters(n) → 读取配置文件 → 初始化全局参数

* trackerData.stereo_readIntrinsicParameter(CAM_NAMES) → 读取相机内参

* std::thread sync_thread{sync_process} → 启动同步线程

* sync_process() → 同步左右相机图像

* getImageFromMsg() → 转换ROS图像消息为OpenCV格式

* handle_stereo_image() → 处理立体图像

* trackerData.trackImage() → 特征跟踪核心函数


### &#9889;2. stereo_event_tracker_node的main函数

基本同上

### &#9889;3. sync_process()事件相机函数

值得学习的是对事件的处理方式

首先说明，对于事件队列：

`这是左右相机事件的定义：queue<dvs_msgs::EventArray> events_left_buf;`


队列（queue）：这是C++标准库中的一个容器适配器，实现了FIFO（先进先出）的数据结构。主要操作有：

    push：添加元素到队尾
    pop：移除队首元素
    front：访问队首元素
    empty：检查队列是否为空




dvs_msgs::EventArray：这是一个ROS消息类型，用于存储事件相机捕获的事件数据。从你提供的头文件可以看出它包含以下字段：

    header：标准ROS消息头，包含时间戳和坐标帧ID
    height：图像高度（行数）
    width：图像宽度（列数）
    events：事件数组，类型为Event[]



Event 结构：根据定义，每个Event包含：

    x：事件在图像上的x坐标（列）
    y：事件在图像上的y坐标（行）
    ts：事件的时间戳
    polarity：事件的极性（true或false，表示亮度增加或减少）

所以，queue<dvs_msgs::EventArray> events_left_buf 是一个队列，用于存储来自左相机的事件数据包。每个数据包（EventArray）包含在特定时间窗口内捕获的一系列事件。

在sync_process函数中，每次循环只处理队列中的一个dvs_msgs::EventArray消息

```
void sync_process()
{
    while(1)
    {
            dvs_msgs::EventArray event_left, event_right;
            std_msgs::Header header;
            double msg_timestamp = 0.0;

            m_buf_event.lock(); // 锁定事件缓冲区互斥量，防止在访问缓冲区时发生竞争条件

            // 检查左右事件缓冲区是否都不为空
            if (!events_left_buf.empty() && !events_right_buf.empty()){
                // 获取左右事件队列前端的时间戳
                double time_left = events_left_buf.front().header.stamp.toSec();
                double time_right = events_right_buf.front().header.stamp.toSec();
                // 使用左相机事件的时间戳作为消息时间戳
                msg_timestamp = time_left;

                // 如果左相机事件时间戳比右相机早0.2秒以上，则丢弃左相机事件
                if(time_left < time_right - 0.2) //tolerance
                {
                    events_left_buf.pop();
                    printf("throw events1\n");
                }
                // 如果左相机事件时间戳比右相机晚0.2秒以上，则丢弃右相机事件
                else if(time_left > time_right + 0.2)
                {
                    events_right_buf.pop();
                    printf("throw events2\n");
                }
                // 如果两个事件的时间戳差距在0.2秒内，则认为它们是同步的
                else
                {
                    // 记录消息时间戳和头部信息
                    msg_timestamp = events_left_buf.front().header.stamp.toSec();
                    header = events_left_buf.front().header;
                    // 获取左相机事件
                    event_left = events_left_buf.front();
                    // 移除处理过的左相机事件
                    events_left_buf.pop();
                    // 获取右相机事件
                    event_right = events_right_buf.front();
                    // 移除处理过的右相机事件
                    events_right_buf.pop();
                }

            }
            // 解锁互斥量
            m_buf_event.unlock();

            if(event_left.events.size()!= 0){

                handle_stereo_event(event_left, event_right, msg_timestamp);

            }

        std::chrono::milliseconds dura(2);
        std::this_thread::sleep_for(dura);
    }
    

}
```


### &#9889;4. handle_stereo_event函数

在trackerData.trackEvent()函数中，完成左右相机特征点的检测和匹配，匹配的结果保存在trackerData对象中，包括：

    trackerData.ids: 左相机特征点的ID
    trackerData.ids_right: 右相机特征点的ID
    以及相应的位置、速度等信息

当同一个特征点在左右相机中都被检测到时，它们被赋予相同的特征ID，在`hash_ids`集合中保存了左相机所有特征点的ID，通过检查右相机特征点的ID是否在hash_ids中存在，可以确定这个特征点是否在两个相机中都被检测到

具体实现逻辑

  * 特征跟踪和匹配：在trackerData.trackEvent()函数中，对左右相机的事件数据进行处理，检测特征点并进行双目匹配（基于特征描述子、空间位置或极线约束等），成功匹配的特征点被赋予相同的ID。

  * ID分配机制：新检测到的特征点获得新的唯一ID，匹配成功的左右相机特征点共享同一个ID，这些ID被分别存储在trackerData.ids和trackerData.ids_right中。

  * 发布前的过滤：代码首先处理左相机的特征点，并将其ID存入hash_ids，然后处理右相机的特征点，但只保留那些ID在hash_ids中存在的点，确保只发布双目匹配成功的特征点对。


```
void handle_stereo_event(const dvs_msgs::EventArray &event_left, const dvs_msgs::EventArray &event_right, double msg_timestamp)
{   
    static int cnt = 0;
    const int n_event = event_left.events.size();

    if(n_event == 0){
        ROS_WARN("not event, please move the event camera or check whether connecting");
        return;
    }

    if(first_image_flag)// 处理第一帧事件的特殊情况
    {
        first_image_flag = false;
        first_image_time = msg_timestamp;
        last_image_time = msg_timestamp;
        return;
    }

    // 检测事件流是否连续，如果时间戳异常则重置跟踪器
    if (msg_timestamp - last_image_time > 1.0 || msg_timestamp < last_image_time)
    {
        ROS_WARN("event stream discontinue! reset the event feature tracker!");
        first_image_flag = true; 
        last_image_time = 0;
        pub_count = 1;
        std_msgs::Bool restart_flag;
        restart_flag.data = true;
        pub_restart.publish(restart_flag);// reset para and reset operation
        return;
    }
    last_image_time = msg_timestamp;

    // frequency control
    if (round(1.0 * pub_count / (msg_timestamp - first_image_time)) <= FREQ) // ?这里的msg_timestamp 还有first_image_time的赋值逻辑
    {
        PUB_THIS_FRAME = true;// 标记当前帧需要发布

        // 当达到目标频率时重置计时
        if (abs(1.0 * pub_count / (msg_timestamp - first_image_time) - FREQ) < 0.01 * FREQ)
        {
            first_image_time = msg_timestamp;
            pub_count = 0;
        }
    }
    else
        PUB_THIS_FRAME = false;

    double msg_timestamp_left = event_left.events.back().ts.toSec();

    if (Do_motion_correction == 0){
        // 不进行运动校正，直接跟踪事件
        trackerData.trackEvent(msg_timestamp_left, event_left, event_right);//optiflow track 核心功能4 匹配
    }else{
        Motion_correction_value motion_compensation;// 进行运动校正，需要计算运动补偿参数
        // 获取事件批次的起始和结束时间
        double t_left_0 = event_left.events[0].ts.toSec();  // 第一个事件的时间戳
        double t_left_1 = event_left.header.stamp.toSec();  // 事件批次的时间戳
        
        // 声明用于存储动态信息的变量
        Eigen::Vector3d temp_v;  // 当前速度
        Eigen::Vector3f temp_a;  // 当前加速度
        double temp_time;        // 当前时间
        Eigen::Vector4d State_;  // 保存当前状态和时间
        Eigen::Vector2d t_0_1(t_left_0,t_left_1);  // 事件时间窗口
        Eigen::Vector3f omega_avg_;  // IMU角速度
        Eigen::Vector3f accel_avg_;  // IMU加速度

        // 如果有IMU数据
        if (!imu_buf.empty()){
            // 如果有里程计数据，获取当前速度
            if(!odom_buffer_.empty()){
                temp_time = odom_buffer_.front()->header.stamp.toSec();
                // 获取当前无人机速度
                temp_v[0]=odom_buffer_.front()->twist.twist.linear.x;
                temp_v[1]=odom_buffer_.front()->twist.twist.linear.y;
                temp_v[2]=odom_buffer_.front()->twist.twist.linear.z;
                odom_buffer_.pop();  // 弹出处理过的里程计数据

                // 保存当前状态
                State_[0]=temp_v[0];
                State_[1]=temp_v[1];
                State_[2]=temp_v[2];
                State_[3]=temp_time;

                // 更新前一时刻和当前时刻的速度
                v_pre[0] = v_cur[0];
                v_pre[1] = v_cur[1];
                v_pre[2] = v_cur[2];
                v_cur[0] = temp_v[0];
                v_cur[1] = temp_v[1];
                v_cur[2] = temp_v[2];

                // 更新前一时刻和当前时刻的时间
                t_pre = t_cur;  //t_pre存储上一次获取里程计数据的时间戳
                t_cur = temp_time; //t_cur存储当前获取的里程计数据的时间戳

                // 计算加速度（速度差除以时间差）
                temp_a[0] = (v_cur[0] - v_pre[0])/(t_cur - t_pre);  //_cur - t_pre是连续两次获取里程计数据之间的时间差
                temp_a[1] = (v_cur[1] - v_pre[1])/(t_cur - t_pre);
                temp_a[2] = (v_cur[2] - v_pre[2])/(t_cur - t_pre);
            }

            // 移除事件时间窗口之前的IMU数据
            while(!imu_buf.empty()){
                if (imu_buf.front()->header.stamp.toSec() < t_left_0)
                    imu_buf.pop();
                else
                    break;
            }

            // 获取IMU数据（角速度和线性加速度）
            if (!imu_buf.empty()){
                omega_avg_[0]=imu_buf.front()->angular_velocity.x;
                omega_avg_[1]=imu_buf.front()->angular_velocity.y;
                omega_avg_[2]=imu_buf.front()->angular_velocity.z;
                accel_avg_[0]=imu_buf.front()->linear_acceleration.x;
                accel_avg_[1]=imu_buf.front()->linear_acceleration.y;
                accel_avg_[2]=imu_buf.front()->linear_acceleration.z-9.805;  // 减去重力加速度
            }
        }
        
        // 构建运动补偿数据结构 ？实现方法
        motion_compensation = std::make_pair(is_nolinear,std::make_pair(std::make_pair (State_,v_pre),std::make_pair(t_0_1,std::make_pair(temp_a,omega_avg_))));

        // 使用运动补偿参数跟踪事件
        trackerData.trackEvent(msg_timestamp_left, event_left, event_right, motion_compensation);
    }

    if (SHOW_TRACK)//show the tracking process
        {
            cv::Mat imageTrack=trackerData.getTrackImage();
            cv::Mat imgTrack_two =trackerData.getTrackImage_two();
            cv::Mat imgTrack_two_point =trackerData.getTrackImage_two_point();
            cv::Mat Time_surface_map =trackerData.gettimesurface();
            cv::Mat event_loop = trackerData.getEventloop();
            pubTrackImage(imageTrack,imgTrack_two,imgTrack_two_point,Time_surface_map,last_image_time,event_loop);
        }


   // 如果需要发布当前帧的特征点 创建和填充了一个ROS点云消息，用于传输特征点信息
    if (PUB_THIS_FRAME)
    {
        pub_count++;  // 更新发布计数

        {
            // 创建点云消息，用于存储特征点 智能指针，指向新分配的PointCloud消息对象 PointCloud是ROS中传输3D点集合的标准消息类型
            sensor_msgs::PointCloudPtr feature_points(new sensor_msgs::PointCloud);
            // 创建特征点的各种属性通道
            sensor_msgs::ChannelFloat32 id_of_point;         // 特征点ID
            sensor_msgs::ChannelFloat32 u_of_point;          // 特征点u坐标
            sensor_msgs::ChannelFloat32 v_of_point;          // 特征点v坐标
            sensor_msgs::ChannelFloat32 velocity_x_of_point; // 特征点x方向速度
            sensor_msgs::ChannelFloat32 velocity_y_of_point; // 特征点y方向速度

            // 设置点云消息的时间戳和坐标系
            feature_points->header.stamp = ros::Time(msg_timestamp);
            feature_points->header.frame_id = "world";

            int camera_id = 0;  // 左相机ID为0
            int feature_id;
            geometry_msgs::Point32 p;
            
            // 处理左图像上的特征点
            set<int> hash_ids;  // 用于跟踪已处理的特征点ID
            for(size_t j=0; j<trackerData.ids.size(); j++){
               // 只处理被跟踪超过1帧的特征点
               if (trackerData.track_cnt[j] > 1)
                {
                    feature_id = trackerData.ids[j];
                    // 归一化平面上的坐标
                    p.x = trackerData.cur_un_pts[j].x;
                    p.y = trackerData.cur_un_pts[j].y;
                    p.z = 1;
                    
                    // 记录已处理的特征点ID
                    hash_ids.insert(feature_id);
                    // 添加特征点到点云
                    feature_points->points.push_back(p);
                    // 添加特征点属性
                    id_of_point.values.push_back(feature_id * NUM_OF_CAM_stereo + camera_id);
                    u_of_point.values.push_back(trackerData.cur_pts[j].x);
                    v_of_point.values.push_back(trackerData.cur_pts[j].y);
                    velocity_x_of_point.values.push_back(trackerData.pts_velocity[j].x);
                    velocity_y_of_point.values.push_back(trackerData.pts_velocity[j].y);
                } 
            }
            
            // 处理右图像上的特征点
            camera_id = 1;  // 右相机ID为1
            for(size_t j=0; j<trackerData.ids_right.size(); j++){
               feature_id = trackerData.ids_right[j];
               // 只处理在左图像中也被跟踪的特征点（双目匹配点）trackEvent函数已经完成了匹配
               if (hash_ids.find(feature_id) != hash_ids.end())
                {
                    // 归一化平面上的坐标
                    p.x = trackerData.cur_un_right_pts[j].x;
                    p.y = trackerData.cur_un_right_pts[j].y;
                    p.z = 1;
                    // 添加特征点到点云
                    feature_points->points.push_back(p);
                    // 添加特征点属性
                    id_of_point.values.push_back(feature_id * NUM_OF_CAM_stereo + camera_id);
                    u_of_point.values.push_back(trackerData.cur_right_pts[j].x);
                    v_of_point.values.push_back(trackerData.cur_right_pts[j].y);
                    velocity_x_of_point.values.push_back(trackerData.right_pts_velocity[j].x);
                    velocity_y_of_point.values.push_back(trackerData.right_pts_velocity[j].y);
                } 
            }
            
            // 将所有属性通道添加到点云消息
            feature_points->channels.push_back(id_of_point);
            feature_points->channels.push_back(u_of_point);
            feature_points->channels.push_back(v_of_point);
            feature_points->channels.push_back(velocity_x_of_point);
            feature_points->channels.push_back(velocity_y_of_point);

            // 跳过第一帧（因为第一帧没有光流速度）
            if (!init_pub)
            {
                init_pub = 1;
            }
            else
                // 发布特征点消息到姿态图优化节点
                pub_img.publish(feature_points);
        }
    }
}
```


### &#9889;5. FeatureTracker::trackImage函数

trackImage函数接收当前时间戳和左右相机图像，跟踪特征点的运动，并在需要时检测新的特征点。它实现了：

* 时间上的特征跟踪（前后帧之间）
* 空间上的特征匹配（左右相机之间）


**时间上的特征跟踪（前后帧）**：使用LK光流法跟踪特征点从前一帧到当前帧的运动,可选的反向光流检查提高跟踪可靠性,移除边界外和跟踪失败的点

**特征点检测**：当特征点数量不足时，在合适区域检测新特征点, 使用掩码确保新特征点与现有点保持足够距离, 为新特征点分配唯一ID和初始跟踪计数

**空间上的特征匹配（左右相机）**：使用光流法在右图像中寻找左图像特征点的对应点, 通过反向光流检查验证匹配的可靠性, 共享特征点ID（ids_right = ids）实现左右图像特征点的关联

**特征点处理**：计算归一化坐标（去除镜头畸变）;计算特征点速度（光流速度）;维护特征点的跟踪计数和映射表

整个函数实现了一个完整的双目特征跟踪系统，既能在时间上跟踪特征点的运动，又能在空间上实现左右相机的特征匹配


**一些非常重要的细节：**

`cv::goodFeaturesToTrack()`函数: OpenCV中实现Shi-Tomasi角点检测器的函数。

Shi-Tomasi角点检测器的使用逻辑与光流跟踪的顺序: 不是所有帧都检测新角点：只有当PUB_THIS_FRAME为真（基于频率控制）且现有特征点数量不足（小于MAX_CNT_IMG）时，才会检测新角点。已跟踪的特征点优先：系统优先使用光流法跟踪前一帧的特征点到当前帧，仅在现有特征点数量不足时，才在当前帧中检测新角点。

混合使用策略：当前帧的特征点包括两部分：从前一帧跟踪过来的旧特征点和在当前帧新检测的特征点。



```
void FeatureTracker::trackImage(double _cur_time, const cv::Mat &img_left, const cv::Mat &img_right)
{   
    TicToc t_r; // 计时器，用于测量函数执行时间

    cur_time = _cur_time; // 保存当前时间戳到类成员变量

    int row = img_left.rows;
    int col = img_left.cols;

    cv::Mat rightImg = img_right;

    // 如果当前图像为空（首次调用），初始化前一帧和当前帧为传入的左图像
    if(cur_img_left.empty()){
        prev_img_left = cur_img_left = img_left; 
    }else{
        cur_img_left = img_left;  // 否则，更新当前帧
    }

    cur_pts.clear(); // 清空当前特征点容器，准备存储新的跟踪结果
    
    if (prev_pts.size() > 0) // 如果前一帧有特征点，则进行特征跟踪
    {
        TicToc t_o; // 计时器，用于测量光流计算时间

        // 状态向量和误差向量，用于存储光流跟踪结果
        vector<uchar> status;
        vector<float> err;
        
        // 使用金字塔LK光流法跟踪前一帧到当前帧的特征点
        // 参数：前一帧图像，当前帧图像，前一帧特征点，当前帧特征点（输出），状态标志，误差，窗口大小，金字塔层数
        cv::calcOpticalFlowPyrLK(prev_img_left, cur_img_left, prev_pts, cur_pts, status, err, cv::Size(21, 21), 3);

        // 如果启用了反向光流检查（提高跟踪可靠性）
        if(FLOW_BACK)
        {
            // 反向状态向量和反向特征点（初始化为前一帧特征点）
            vector<uchar> reverse_status;
            vector<cv::Point2f> reverse_pts = prev_pts;
            
            // 计算反向光流，从当前帧返回前一帧
            cv::calcOpticalFlowPyrLK(cur_img_left, prev_img_left, cur_pts, reverse_pts, reverse_status, err, cv::Size(21, 21), 3);
            
            // 遍历所有特征点，验证正反向光流的一致性
            for(size_t i = 0; i < status.size(); i++)
            {
                // 如果正向跟踪成功、反向跟踪成功、且反向跟踪回到的位置与原位置距离小于0.5像素
                if(status[i] && reverse_status[i] && distance(prev_pts[i], reverse_pts[i]) <= 0.5)
                {
                    // 保持该点的状态为有效
                    status[i] = 1;
                }
                else
                    // 否则标记为无效
                    status[i] = 0;
            }
        }
        
        // 检查特征点是否在图像边界内
        for (int i = 0; i < int(cur_pts.size()); i++)
            if (status[i] && !inBorder(cur_pts[i]))
                status[i] = 0;  // 如果超出边界，标记为无效
        
        // 根据状态标志移除无效的特征点及其相关信息
        reduceVector(prev_pts, status);  // 移除无效的前一帧特征点
        reduceVector(cur_pts, status);   // 移除无效的当前帧特征点
        reduceVector(ids, status);       // 移除无效的特征点ID
        reduceVector(track_cnt, status); // 移除无效的跟踪计数
    }

    for (auto &n : track_cnt) // 对所有成功跟踪的特征点，增加其跟踪计数
        n++;
    
    if (PUB_THIS_FRAME) // 如果当前帧需要发布（标志位置位在handle_stereo_image函数频率控制部分中）
    {        
        TicToc t_m; // 计时器，用于测量掩码设置时间
        Image_setMask(); //核心功能 设置掩码，避免新特征点与现有点太近

        TicToc t_t; // 计时器，用于测量特征检测时间

        // 计算需要检测的新特征点数量，确保总数不超过最大值
        int n_max_cnt = MAX_CNT_IMG - static_cast<int>(cur_pts.size());

        // 如果需要检测新特征点
        if (n_max_cnt > 0)
        {
            // 检查掩码是否准备好
            if(mask_image.empty())
                cout << "mask is empty " << endl;
            if (mask_image.type() != CV_8UC1)
                cout << "mask type wrong " << endl;

            // 使用Shi-Tomasi角点检测器在掩码允许的区域检测新特征点
            // 参数：输入图像，输出点，最大点数，质量阈值，最小距离，掩码    
            cv::goodFeaturesToTrack(cur_img_left, n_pts, MAX_CNT_IMG - cur_pts.size(), 0.01, MIN_DIST_IMG, mask_image);

        }
        else
            n_pts.clear(); // 如果不需要新特征点，清空容器
         
        for (auto &p : n_pts) // 为每个新检测的特征点分配ID和初始跟踪计数
        {
            cur_pts.push_back(p);       // 添加到当前特征点列表
            ids.push_back(n_id++);      // 分配唯一ID并递增ID计数器
            track_cnt.push_back(1);     // 初始化跟踪计数为1
        }
    }
    
    cur_un_pts.clear(); // 清空归一化特征点容器
    cur_un_pts = undistortedPts(cur_pts, stereo_m_camera[0]); // 计算特征点的归一化坐标（去畸变）todo
    pts_velocity.clear(); // 清空特征点速度容器
    pts_velocity = ptsVelocity(ids, cur_un_pts, cur_un_pts_map, prev_un_pts_map); // 计算特征点的速度（基于前后帧的位置变化）todo ？计算的是一整个序列的速度吗

    // 如果右图像不为空，处理双目特征匹配
    if(!img_right.empty())
    {
        ids_right.clear();
        cur_right_pts.clear();
        cur_un_right_pts.clear();
        right_pts_velocity.clear();
        cur_un_right_pts_map.clear();

        // 如果左图像有特征点
        if(!cur_pts.empty())
        {
            // 用于存储反向检查的点
            vector<cv::Point2f> reverseLeftPts;
            
            // 状态向量和误差向量
            vector<uchar> status, statusRightLeft;
            vector<float> err;
            
            // 使用光流法在右图像中寻找左图像特征点的对应点
            // 参数：左图像，右图像，左图像特征点，右图像特征点（输出），状态标志，误差，窗口大小，金字塔层数
            cv::calcOpticalFlowPyrLK(cur_img_left, rightImg, cur_pts, cur_right_pts, status, err, cv::Size(21, 21), 3);
            
            // 如果启用反向检查且右图像特征点不为空
            if(FLOW_BACK && !cur_right_pts.empty())
            {
                // 计算从右图像到左图像的反向光流
                cv::calcOpticalFlowPyrLK(rightImg, cur_img_left, cur_right_pts, reverseLeftPts, statusRightLeft, err, cv::Size(21, 21), 3);
                
                // 遍历所有特征点，验证正反向光流的一致性
                for(size_t i = 0; i < status.size(); i++)
                {
                    // 如果正向匹配成功、反向匹配成功、右图点在边界内、且反向匹配回到的位置与原位置距离小于0.5像素
                    if(status[i] && statusRightLeft[i] && inBorder(cur_right_pts[i]) && distance(cur_pts[i], reverseLeftPts[i]) <= 0.5)
                    {
                        // 保持该点的状态为有效
                        status[i] = 1;
                    }
                    else
                        // 否则标记为无效
                        status[i] = 0;
                }
            }

            // //fundamental matrix check
            
                // vector<cv::Point2f> un_l_pts, un_r_pts; 
                // un_l_pts.reserve(ids.size()); 
                // un_r_pts.reserve(ids.size()); 

                // for(int i=0; i<ids.size(); i++){
                //     if(status[i]){
                //         Eigen::Vector3d tmp_p;
                //         stereo_m_camera[0]->liftProjective(Eigen::Vector2d(cur_pts[i].x, cur_pts[i].y), tmp_p);
                //         // ouf2 << i<<" l: "<< tmp_p.transpose()<<" "; 
                //         // tmp_p.x() = FOCAL_LENGTH * tmp_p.x() / tmp_p.z() + COL / 2.0;
                //         // tmp_p.y() = FOCAL_LENGTH * tmp_p.y() / tmp_p.z() + ROW / 2.0;
                //         un_l_pts.push_back(cv::Point2f(tmp_p.x(), tmp_p.y())); 
                //         // ouf2<<" proj: "<<tmp_p.x()<<" "<<tmp_p.y()<<" "<<endl; 

                //         stereo_m_camera[1]->liftProjective(Eigen::Vector2d(cur_right_pts[i].x, cur_right_pts[i].y), tmp_p);
                //         // ouf2 << i<<" r: "<< tmp_p.transpose()<<" "; 
                //         // tmp_p.x() = FOCAL_LENGTH * tmp_p.x() / tmp_p.z() + COL / 2.0;
                //         // tmp_p.y() = FOCAL_LENGTH * tmp_p.y() / tmp_p.z() + ROW / 2.0;
                //         un_r_pts.push_back(cv::Point2f(tmp_p.x(), tmp_p.y()));
                //         // ouf2<<" proj: "<<tmp_p.x()<<" "<<tmp_p.y()<<" "<<endl; 
                //         // ouf<<i<<" "<<cur_pts[i].x<<" "<<cur_pts[i].y<<" "<<cur_right_pts[i].x<<" "<<cur_right_pts[i].y<<endl;
                //     }
                // }

            // // use Tlr to to verification
            
            // 关键步骤：右图像特征点继承左图像对应点的ID
            // 这确保了左右图像中匹配的特征点共享相同的ID
            ids_right = ids;

            // 根据状态标志移除无效的右图像特征点及其相关信息
            reduceVector(cur_right_pts, status);  // 移除无效的右图像特征点
            reduceVector(ids_right, status);      // 移除无效的右图像特征点ID
            reduceVector(track_cnt_right, status); // 移除无效的右图像特征点跟踪计数

            // 对所有成功匹配的右图像特征点，增加其跟踪计数
            for (auto &n : track_cnt_right)
                n++;

            // 为每个右图像特征点初始化跟踪计数
            // 注意：这里可能有逻辑问题，因为已经增加了现有点的计数，再为所有点添加计数=1可能不合理
            for (auto &p : cur_right_pts)
            {
                track_cnt_right.push_back(1);
            }

            // 计算右图像特征点的归一化坐标（去畸变）
            cur_un_right_pts = undistortedPts(cur_right_pts, stereo_m_camera[1]);
            
            // 计算右图像特征点的速度
            right_pts_velocity = ptsVelocity(ids_right, cur_un_right_pts, cur_un_right_pts_map, prev_un_right_pts_map);
        }
        
        // 更新前一帧右图像的归一化点映射
        prev_un_right_pts_map = cur_un_right_pts_map;
    }

    // 如果需要显示跟踪结果
    if(SHOW_TRACK)
        // 绘制跟踪和匹配的特征点
        drawTrack(cur_img_left, rightImg, ids, cur_pts, cur_right_pts, prevLeftPtsMap);

    // 更新前一帧的图像、特征点和时间戳，为下一次跟踪做准备
    prev_img_left = cur_img_left;
    prev_pts = cur_pts;
    prev_un_pts = cur_un_pts;
    prev_un_pts_map = cur_un_pts_map;
    prev_time = cur_time;

    // 更新前一帧左图像的特征点ID映射表（ID到坐标的映射）
    prevLeftPtsMap.clear();
    for(size_t i = 0; i < cur_pts.size(); i++)
        prevLeftPtsMap[ids[i]] = cur_pts[i];
    
    return; // 函数结束
}
```




### &#9889;6. FeatureTracker::trackEvent函数（非运动补偿）

函数形式：`void FeatureTracker::trackEvent(double _cur_time, const dvs_msgs::EventArray &event_left, const dvs_msgs::EventArray &event_right)`

与图像跟踪不同的一点在于使用自定义的`Event_FeaturesToTrack函数`补充特征点，这个将在后面详细介绍

几个小问题：

1. 什么是当前帧左图像特征点的ID映射表（ID到坐标的映射）？

   ID映射表类型：map<int, cv::Point2f> curLeftPtsMap;

   ID映射表是一个数据结构，将特征点的唯一标识符(ID)映射到其在图像中的坐标位置。

2. 什么是特征点的归一化坐标（去畸变）？形式是什么样的？

   归一化坐标是指将图像中的特征点坐标转换为去除镜头畸变后的标准化坐标。

   在代码中，这是通过`undistortedPts函数`实现的：

3. 两帧之间匹配的特征点对是以什么形式返回的？

   在这个实现中，两帧之间匹配的特征点对不是作为单独的数据结构直接返回的，而是通过几个平行数组隐式表示。

   核心数据结构：

        prev_pts和cur_pts：前一帧和当前帧中对应特征点的坐标数组
        ids：特征点的唯一标识符数组
        status：表示每对特征点匹配是否有效的布尔数组

    匹配形式：

        相同索引对应关系：prev_pts[i]与cur_pts[i]构成一对匹配的特征点，它们共享相同的ID ids[i]
        数组大小一致：经过reduceVector函数处理后，这些数组大小相同，且只包含有效匹配


在`handle_stereo_event函数`中：

```
if (Do_motion_correction == 0)
// 不进行运动校正，直接跟踪事件
trackerData.trackEvent(msg_timestamp_left, event_left, event_right);
```

这里调用了trackEvent进行匹配，处理后的匹配结果主要保存在 `FeatureTracker`类的多个成员变量中，这些变量在函数执行过程中被更新。主要包括：

1. 特征点坐标和关联信息

    `cur_pts`: 当前左图像中的特征点坐标

    `cur_right_pts`: 当前右图像中的特征点坐标

    `ids`: 左图像特征点的唯一ID
    
    `ids_right`: 右图像特征点的唯一ID（与左图对应）

    `track_cnt`: 每个特征点被连续跟踪的帧数

    `track_cnt_right`: 右图像特征点被连续跟踪的帧数

2. 归一化坐标和速度信息

    `cur_un_pts`: 当前左图像特征点的归一化坐标（去畸变）

    `cur_un_right_pts`: 当前右图像特征点的归一化坐标

    `pts_velocity`: 左图像特征点的速度

    `right_pts_velocity`: 右图像特征点的速度

3. 映射关系

    `curLeftPtsMap`: 当前左图像特征点ID到坐标的映射

    `curRightPtsMap`: 当前右图像特征点ID到坐标的映射

    `prevLeftPtsMap`: 保存当前左图像特征点信息，下一帧将作为前一帧的参考

在stereo_event_tracker_node.cpp中开头定义了全局变量trackerData, 函数可以直接读取trackerData的成员变量，因此跟踪函数调用以后能够进一步得到匹配的特征点对。



```
void FeatureTracker::trackEvent(double _cur_time, const dvs_msgs::EventArray &event_left, const dvs_msgs::EventArray &event_right)
{
    // 保存当前时间戳到类成员变量
    cur_time = _cur_time;
    
    // 声明用于存储时间面图像的变量
    cv::Mat img_left;
    cv::Mat img_right;
    cv::Mat rightImg; // 用于绘制跟踪结果的右图像

    // 如果检测器尚未初始化，进行初始化
    // FLAG_DETECTOR_NOSTART是一个标志，表示检测器是否已启动
    if(FLAG_DETECTOR_NOSTART){
        FLAG_DETECTOR_NOSTART = false;
        // 使用事件图像的列数和行数初始化检测器  核心，这里与event_detector.cpp关联起来了
        detector.init(COL_event, ROW_event);
    }

    // 创建左右相机的事件矩阵（Surface of Active Events，SAE）
    // 初始化为全零的彩色图像
    detector.cur_event_mat_left = cv::Mat::zeros(cv::Size(COL_event, ROW_event), CV_8UC3);
    detector.cur_event_mat_right = cv::Mat::zeros(cv::Size(COL_event, ROW_event), CV_8UC3);
    
    TicToc t_SAE;// 计时器，用于测量SAE创建的时间

    // 遍历左相机的所有事件，更新左相机的SAE
    for (const dvs_msgs::Event& e_left:event_left.events){
        // 调用检测器的createSAE_left方法，传入事件的时间戳、坐标和极性
        detector.createSAE_left(e_left.ts.toSec(), e_left.x, e_left.y, e_left.polarity);
    }

    // 遍历右相机的所有事件，更新右相机的SAE
    for (const dvs_msgs::Event& e_right:event_right.events){
        detector.createSAE_right(e_right.ts.toSec(), e_right.x, e_right.y, e_right.polarity);
    }

    // 获取当前左右相机的事件矩阵（SAE）
    cv::Mat event_mat_left = detector.cur_event_mat_left;
    cv::Mat event_mat_right = detector.cur_event_mat_right;

    // 将SAE转换为时间面（Time Surface）
    // 时间面是表示每个像素最近事件发生时间的图像
    const cv::Mat time_surface_map_left = detector.SAEtoTimeSurface_left(cur_time);
    const cv::Mat time_surface_map_right = detector.SAEtoTimeSurface_right(cur_time);

    // 复制时间面用于可视化
    time_surface_visualization_left = time_surface_map_left.clone();
    time_surface_visualization_right = time_surface_map_right.clone();
    
    // 复制时间面用于特征跟踪
    cv::Mat time_surface_left = time_surface_map_left.clone();
    cv::Mat time_surface_right = time_surface_map_right.clone();
    
    // 如果启用了均衡化处理
    if (EQUALIZE)
    {
        // 创建对比度受限的自适应直方图均衡化（CLAHE）对象
        cv::Ptr<cv::CLAHE> clahe = cv::createCLAHE();
        
        // 对左右时间面应用CLAHE
        clahe->apply(time_surface_left, img_left);
        clahe->apply(time_surface_right, img_right);
        
        // 将结果归一化到0-255范围
        cv::normalize(img_left, img_left, 0, 255, CV_MINMAX);
        cv::normalize(img_right, img_right, 0, 255, CV_MINMAX);
    }
    else
    {
        // 如果不均衡化，直接使用原始时间面
        img_left = time_surface_left;
        img_right = time_surface_right;
    }


    // 如果当前左图像为空（首次调用），初始化前一帧和当前帧
    if(cur_img_left.empty()){
        prev_img_left = cur_img_left = img_left; 
        prev_event_mat_left = event_mat_left; 
    }else{
        // 否则，更新当前左图像
        cur_img_left = img_left; 
    }
    
    // 清空当前左图像特征点容器
    cur_pts.clear();

    // 如果当前右图像为空（首次调用），初始化前一帧和当前帧
    if(cur_img_right.empty()){
        prev_img_right = cur_img_right = img_right;
    }else{
        // 否则，更新当前右图像
        cur_img_right = img_right;
    }
    
    // 清空当前右图像特征点容器
    cur_right_pts.clear();
    
    if (prev_pts.size() > 0) // 如果前一帧有特征点，则进行特征跟踪
    {   
        // 计时器，用于测量光流计算时间
        TicToc t_o;
        
        // 状态向量和误差向量，用于存储光流跟踪结果
        vector<uchar> status;
        vector<float> err;
        // 使用金字塔LK光流法跟踪前一帧到当前帧的特征点
        cv::calcOpticalFlowPyrLK(prev_img_left, cur_img_left, prev_pts, cur_pts, status, err, cv::Size(21, 21), 3);

        // 如果启用了反向光流检查
        if(FLOW_BACK)
        {
            vector<uchar> reverse_status;
            vector<cv::Point2f> reverse_pts = prev_pts;
            
            // 计算反向光流，从当前帧返回前一帧
            // 注意：这里使用了更多参数，包括终止条件和初始流使用标志
            cv::calcOpticalFlowPyrLK(cur_img_left, prev_img_left, cur_pts, reverse_pts, reverse_status, err, cv::Size(21, 21), 1, 
            cv::TermCriteria(cv::TermCriteria::COUNT+cv::TermCriteria::EPS, 30, 0.01), cv::OPTFLOW_USE_INITIAL_FLOW);
            
            // 遍历所有特征点，验证正反向光流的一致性
            for(size_t i = 0; i < status.size(); i++)
            {
                // 如果正向跟踪成功、反向跟踪成功、且反向跟踪回到的位置与原位置距离小于0.5像素
                if(status[i] && reverse_status[i] && distance(prev_pts[i], reverse_pts[i]) <= 0.5)
                {
                    // 保持该点的状态为有效
                    status[i] = 1;
                }
                else
                    // 否则标记为无效
                    status[i] = 0;
            }
        }
        
        // 检查特征点是否在事件图像边界内
        for (int i = 0; i < int(cur_pts.size()); i++)
            if (status[i] && !inBorder_event(cur_pts[i]))
                status[i] = 0;  // 如果超出边界，标记为无效
        
        // 根据状态标志移除无效的特征点及其相关信息
        reduceVector(prev_pts, status);
        reduceVector(cur_pts, status);
        reduceVector(ids, status);
        reduceVector(track_cnt, status);
    }

    for (auto &n : track_cnt) // 对所有成功跟踪的特征点，增加其跟踪计数
        n++;

    // 如果当前帧需要发布
    if (PUB_THIS_FRAME)
    {
        // 使用基本矩阵RANSAC拒绝外点
        rejectWithF_event();//核心 todo
        
        // 计时器，用于测量掩码设置时间
        TicToc t_m;
        
        // 设置事件图像的掩码，避免新特征点与现有点太近
        Event_setMask(); //核心 todo

        // 计算需要检测的新特征点数量
        int n_max_cnt = MAX_CNT - static_cast<int>(cur_pts.size());

        // 如果需要检测新特征点
        if (n_max_cnt > 0)
        {   
            // 检查掩码是否准备好
            if(mask_event.empty())
                cout << "the time surface mask is empty " << endl;
            if (mask_event.type() != CV_64FC1)
                cout << "time surface mask type wrong " << endl;
            if (mask_event.size() != time_surface_left.size())
                cout << "wrong size" << endl;
            
            // 在事件数据中检测新特征点
            // 与传统图像不同，这里使用自定义的Event_FeaturesToTrack函数 核心 todo
            Event_FeaturesToTrack(event_left, n_pts, cur_pts, MAX_CNT - cur_pts.size(), MIN_DIST, mask_event, time_surface_left);
        }
        else
            // 如果不需要新特征点，清空容器
            n_pts.clear();

        // 为每个新检测的特征点分配ID和初始跟踪计数
        for (auto &p : n_pts)
        {
            cur_pts.push_back(p);
            ids.push_back(n_id++);
            track_cnt.push_back(1);
        }
    }

    // 清空当前归一化特征点容器
    cur_un_pts.clear();
    
    // 计算特征点的归一化坐标（去畸变）
    cur_un_pts = undistortedPts(cur_pts, stereo_m_camera[0]);  //核心 todo
    
    // 清空特征点速度容器
    pts_velocity.clear();
    
    // 计算特征点的速度
    pts_velocity = ptsVelocity(ids, cur_un_pts, cur_un_pts_map, prev_un_pts_map);  //核心 todo

    // 如果右图像不为空，处理双目特征匹配
    if(!img_right.empty())
    {
        // 清空右图像相关的容器
        ids_right.clear();
        cur_right_pts.clear();
        cur_un_right_pts.clear();
        right_pts_velocity.clear();
        cur_un_right_pts_map.clear();
        track_cnt_right.clear();

        if(!cur_pts.empty())// 如果左图像有特征点
        {
            vector<cv::Point2f> reverseLeftPts;// 用于存储反向检查的点

            // 状态向量和误差向量
            vector<uchar> status, statusRightLeft;
            vector<float> err;
            // 使用光流法在右图像中寻找左图像特征点的对应点
            cv::calcOpticalFlowPyrLK(cur_img_left, cur_img_right, cur_pts, cur_right_pts, status, err, cv::Size(21, 21), 3);
      
            // // reverse check cur right ---- cur left matching
            if(FLOW_BACK && !cur_right_pts.empty()) // 如果启用反向光流检查且右图像特征点不为空
            {
                // 计算从右图像到左图像的反向光流
                cv::calcOpticalFlowPyrLK(cur_img_right, cur_img_left, cur_right_pts, reverseLeftPts, statusRightLeft, err, cv::Size(21, 21), 3);
                
                // 遍历所有特征点，验证正反向光流的一致性
                for(size_t i = 0; i < status.size(); i++)
                {
                    // 如果正向匹配成功、反向匹配成功、右图点在边界内、且反向匹配回到的位置与原位置距离小于0.5像素
                    if(status[i] && statusRightLeft[i] && inBorder_event(cur_right_pts[i]) && distance(cur_pts[i], reverseLeftPts[i]) <= 0.5)
                    {
                        status[i] = 1;// 保持该点的状态为有效
                    }
                    else
                        status[i] = 0;// 否则标记为无效
                }
            }

            ids_right = ids;// 右图像特征点继承左图像对应点的ID

            // 根据状态标志移除无效的右图像特征点及其相关信息
            reduceVector(cur_right_pts, status);
            reduceVector(ids_right, status);
            reduceVector(track_cnt_right, status);

            // // fundamental matrix check // // 
            
            //     vector<cv::Point2f> un_l_pts, un_r_pts; 
            //     un_l_pts.reserve(ids.size()); 
            //     un_r_pts.reserve(ids.size()); 

            //     for(int i=0; i<ids.size(); i++){
            //         if(status[i]){
            //             Eigen::Vector3d tmp_p;
            //             stereo_m_camera[0]->liftProjective(Eigen::Vector2d(cur_pts[i].x, cur_pts[i].y), tmp_p);

            //             un_l_pts.push_back(cv::Point2f(tmp_p.x(), tmp_p.y())); 


            //             stereo_m_camera[1]->liftProjective(Eigen::Vector2d(cur_right_pts[i].x, cur_right_pts[i].y), tmp_p);

            //             un_r_pts.push_back(cv::Point2f(tmp_p.x(), tmp_p.y()));
            //         }
            //     }


            // ids_right = ids;
            // reduceVector(cur_right_pts, status);
            // reduceVector(ids_right, status);
            // reduceVector(track_cnt_right, status);

            // assert(cur_right_pts.size() == un_l_pts.size()); 
            // vector<uchar> lr_fund_status(cur_right_pts.size(), 0); 

            // int cnt_right_inlier = 0; 
            // for(int i=0; i<lr_fund_status.size(); i++){
                
            //     // check epipilar distance 
            //     Vector3d p3d0(un_l_pts[i].x, un_l_pts[i].y, 1.);
            //     Vector3d p3d1(un_r_pts[i].x, un_r_pts[i].y, 1.);  
            //     const double epipolar_error =
            //     std::abs(p3d1.transpose() * Eeesntial_matrix_event * p3d0);
            //     if(epipolar_error < 0.005) // epipolar distance 
            //     {
            //         lr_fund_status[i] = 1; 
            //         ++cnt_right_inlier;
            //     }
            // }

            // reduceVector(cur_right_pts, lr_fund_status);
            // reduceVector(ids_right, lr_fund_status);

            // // fundamental matrix check // // 


            // 对所有成功匹配的右图像特征点，增加其跟踪计数
            for (auto &n : track_cnt_right)
                n++;

            // 为每个右图像特征点初始化跟踪计数
            // 注意：这里可能有逻辑问题，因为已经增加了现有点的计数，再为所有点添加计数=1可能不合理
            for (auto &p : cur_right_pts)
            {
                track_cnt_right.push_back(1);
            }

            // 计算右图像特征点的归一化坐标（去畸变）
            cur_un_right_pts = undistortedPts(cur_right_pts, stereo_m_camera[1]);
            
            // 计算右图像特征点的速度
            right_pts_velocity = ptsVelocity(ids_right, cur_un_right_pts, cur_un_right_pts_map, prev_un_right_pts_map);
        }
        
        // 更新前一帧右图像的归一化点映射
        prev_un_right_pts_map = cur_un_right_pts_map;
    }

    // 创建当前帧左图像特征点的ID映射表（ID到坐标的映射）
    curLeftPtsMap.clear();
    for(size_t i = 0; i < cur_pts.size(); i++)
        curLeftPtsMap[ids[i]] = cur_pts[i];

    // 创建当前帧右图像特征点的ID映射表
    curRightPtsMap.clear();
    for(size_t i = 0; i < cur_right_pts.size(); i++)
        curRightPtsMap[ids_right[i]] = cur_right_pts[i];

    // 更新前一帧的图像、特征点和时间戳，为下一次跟踪做准备
    prev_img_left = cur_img_left;
    prev_pts = cur_pts;
    prev_un_pts = cur_un_pts;
    prev_un_pts_map = cur_un_pts_map;
    prev_time = cur_time;
    prev_event_mat_left = event_mat_left;

    // 如果需要显示跟踪结果
    if(SHOW_TRACK){
        // 在事件图像上绘制立体跟踪结果
        stereo_event_drawTrack(event_mat_left, event_mat_right, ids, cur_pts, cur_right_pts, prevLeftPtsMap);
        
        // 在时间面上绘制前后帧跟踪结果
        event_drawTrack_two(cur_img_left, prev_img_left, ids, cur_pts, prev_pts, prevLeftPtsMap);
        
        // 在时间面上绘制左右图像匹配结果
        event_drawTrack_stereo(cur_img_left, cur_img_right, prev_img_left, ids, ids_right, cur_pts, cur_right_pts, curLeftPtsMap, curRightPtsMap);
    }

    // 更新前一帧左图像的特征点ID映射表
    prevLeftPtsMap.clear();
    for(size_t i = 0; i < cur_pts.size(); i++)
        prevLeftPtsMap[ids[i]] = cur_pts[i];

    return; // 函数结束
}
```

### &#9889;7. FeatureTracker::trackEvent函数（运动补偿）

与非运动补偿版的主要区别在于生成SAE的部分调用了的是运动补偿版本的SAE生成函数，这个函数位于event_detector.cc中，下面介绍此函数的运动补偿版本与非运动补偿版本


### &#9889;8. EventDetector::createSAE_left函数（运动补偿）




### &#9889;9. EventDetector::createSAE_left函数（非运动补偿）




### &#9889;10. FeatureTracker::Image_setMask函数




### &#9889;11. FeatureTracker::Event_setMask函数



### &#9889;12. Event_FeaturesToTrack函数




### &#9889;13. FeatureTracker::rejectWithF_event函数





### &#9889;14. vector<cv::Point2f> FeatureTracker::undistortedPts函数
---
layout: post
title: 【translation】ORB-SLAM2:An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras
date: 2023-3-29 
tags: Paper
---

*This is the translation of the [paper](https://arxiv.org/abs/1610.06475)*

# 摘要：

&emsp;&emsp;本文提出了一种能够使用单目相机、双目相机和RGB-D相机的完整SLAM系统，称为ORB-SLAM2。该系统具有地图复用、回环检测和重定位功能。ORB-SLAM2能够实时在标准的中央处理器上运行，并能够在从小型室内手持设备到工业环境中飞行的无人机再到绕城市行驶的汽车等多种环境下工作。该系统基于带有单双目观测的光束平差法（BA）进行后端优化，从而实现精确的轨迹估计。本文所提出的系统包含一个轻量级的定位模式，该模式利用视觉里程计来跟踪未建图区域并匹配地图点以实现零漂移定位。在29个广泛使用的公开数据集下测试的结果表明，本文所提出的SLAM方法能够在多数情况下获得最高的精度。本文开源代码，不仅为了造福SLAM社区，更是为了给其他研究领域的研究人员提供一个可以直接使用的SLAM解决方案。

关键词：定位，建图，RGB-D，同时定位与建图（SLAM），双目

# I. 引言
&emsp;&emsp;同时定位与建图技术（SLAM）在过去的二十年间已经成为计算机视觉和机器人领域中的研究热点，并在近年来吸引着高科技公司的关注。SLAM技术能够建立未知环境的地图并通过高度实时的操作来对地图中的传感器进行定位。在众多传感器中，相机不仅便宜，而且能够为实现鲁棒而准确的位置识别提供丰富的环境信息。因此，以相机为主要传感器的视觉SLAM解决方案成为了主要的研究方向。位置识别是SLAM系统实现回环检测（检测传感器回到了已建图位置并修正建图过程中的累积误差）的关键一环，并且对于实现由陷入闭塞环境或剧烈运动或系统重新初始化导致的轨迹丢失后的重定位起到重要作用。

&emsp;&emsp;视觉SLAM能够通过仅使用一个单目相机实现，这种传感器是最便宜最小的。然而，由于深度信息无法通过单一相机获取，导致了地图规模和估计的轨迹都是不确定的。除此以外，由于不能在初始帧进行三角测量，因此系统自启动需要多视图或滤波器技术来产生一个初始地图。最后，单目SLAM存在尺度漂移问题，如果在传感器探索未知区域的过程中发生了纯旋转则可能会失败。通过使用一个双目相机或RGB-D相机能够解决这些问题并实现最可靠的视觉SLAM。

&emsp;&emsp;本文基于单目ORB-SLAM框架提出ORB-SLAM2，该算法有以下贡献：

1. 第一个具有回环检测、重定位和地图复用功能的开源SLAM系统，并能够使用单目、双目和RGB-D相机实现。
2. RGB-D相机测试结果表明使用光束平差法（BA）获得的结果比基于迭代最近点法（ICP）或广度深度误差最小化法的主流方法更准确。
3. 通过使用远近双目点和单目观测，我们的双目相机测试结果比主流的双目SLAM系统更精确。
4. 轻量级的定位模式使得建图失效时系统能有效对地图进行复用。

&emsp;&emsp;图1给出了ORB-SLAM2系统输入双目和RGB-D相机数据时的输出结果。双目相机结果展现了最终的轨迹和对KITTI数据集中的00数据段的稀疏重建结果。KITTI数据集是一个带有多个回环的城市数据集，ORB-SLAM2能够成功检测到这些回环。RGB-D相机结果展示了TUM RGB-D数据集中的fr1-room数据段的关键帧估计位姿，并给出了根据估计关键帧位姿，对传感器深度图反向投影得到的稠密点云。值得注意的是，本文提出的SLAM方法并没有进行任何形式的融合，如KinectFusion或类似的方法，但是仍然能够精确估计关键帧的位姿。本文算法的更多示例在附件视频中展示。

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig1.jpg" width=400px>
    <center>
    <figcaption>图1. ORB-SLAM2处理输入的双目和RGB-D相机数据来估计降级轨迹并建立环境地图。系统能够在实时CPU上运行并实现高精度和鲁棒性的回环检测、重定位和地图复用。(a) 双目输入下：得到轨迹和对包含多闭环的城市环境的重建结果。（b）RGB-D输入下：得到关键帧和仅有一个闭环的室内场景的点云。点云图是根据关键帧对传感器深度图反向投影渲染得到。</figcaption>
    </center>
</figure>

&emsp;&emsp;下面，本文将在第II部分给出相关工作，在第III部分描述本文所设计的系统，接着在第IV部分给出估计结果，最后在第V部分给出结论。


# II. 相关工作

&emsp;&emsp;本章将讨论关于双目和RGB-D SLAM的相关工作。本章所讨论的内容与第IV章的估计结果仅针对SLAM方法。

## A. 双目SLAM

&emsp;&emsp;Paz等人开发的SLAM系统是早期优秀的双目SLAM方案。该方案基于条件独立分治的扩展卡尔曼滤波SLAM（EKF-SLAM）实现。并且相比于当时的其他方法，这种方案能在更大的环境下运行。最重要的是，它是第一个同时利用近点和远点（即由于双目相机的左右目图像差异小而不能准确估计其深度的点）的双目SLAM方案，并对远点使用逆参数估计。Paz等人基于工程经验提出当空间点的深度小于双目相机基线的40倍时，其才可以被有效三角化。本文将遵循用不同方式处理远近点的策略，具体解释将在III-A部分给出。

&emsp;&emsp;目前，多数的双目SLAM系统都是基于关键帧实现的，并通过局部BA优化获得可扩展性。Strasdat等人在关键帧滑窗内的BA（点-位姿约束）和外部窗口中的位姿图（位姿-位姿约束）进行联合优化。通过限制滑窗的尺寸，这种方法获得了恒定的时间复杂度，但是代价是不能保证全局一致性。Mei等人提出的RSLAM使用了地标和位姿的相对位置表示方法并在活跃区域进行相对BA，这种方法受到恒定时间约束。RSLAM能够实现闭环，这使得它能够在每一个闭环两边扩展活动区域而不影响全局一致性。最近，有Pire等人提出的S-PTAM方法只进行局部BA，它缺少大规模的回环检测。与这些方法相似的是，本文对局部的一系列关键帧进行BA优化以使得复杂度不受地图尺寸的影响并允许ORB-SLAM2能够在大规模环境下运行。然而，本文的目标是建立一个全局一致的地图。与RSLAM相似的是，当发生回环时，本文所设计的系统能够先将地图两端对齐，使追踪线程可以使用之前的地图持续定位，接着进行位姿图优化以减小闭环中的累积漂移。随后进行全局BA。

&emsp;&emsp;Engel等人最近提出的双目大规模直接SLAM（LSD-SLAM）是一种半稠密直接SLAM方法，能够减小高梯度图像区域的光度误差。这种方法希望能够在不依靠图像特征的情况下，当受到动态扰动或处于纹理较少的环境中时能够更加鲁棒。但是由于这是一种直接法，当受到未建模因素例如卷帘快门和非朗伯反射的影响时，其性能会大幅下降。

## B. RGB-D SLAM

&emsp;&emsp;Newcombe等人提出的KinectFusion是最早最著名的RGB-D SLAM系统之一。这种方法将传感器获得的所有深度数据融合成一个体积稠密模型，用于使用ICP法跟踪相机位姿。受限于体积表示和缺少回环检测，该系统只能在小空间范围内运行。Whelan等人提出的Kintinuous通过设置一个滚动循环缓冲区和包含基于位置识别和位姿图优化的回环检测使得系统能够在大规模场景下运行。

&emsp;&emsp;Endres等人提出的RGB-D SLAM方法可能是第一个受到广泛好评的开源SLAM方案。该方法是基于特征点实现的，其前端通过特征匹配和ICP方法计算帧对帧的运动，后端则进行带有回环的位姿优化，回环约束由启发式搜索得到。与之相似的是Kerl等人提出的DVO-SLAM的后端，优化了带有从视觉里程计中计算得到的关键帧对关键帧约束的位姿图，减少了光度和深度的误差。DVO-SLAM同时还可以在之前的所有帧中搜索候选回环，而不需要依赖位置识别。

&emsp;&emsp;最近由Whelan等人提出的ElasticFusion方法建立了基于surfel的环境地图。这是一种忽略位姿而以地图为核心的方法，它采用对地图进行非刚性变形的方式来实现回环检测，而不是采用标准的位姿图优化。该方法具有惊人的细节重建效果和定位精确度，但是由于复杂度会随surfel的数量增加而大幅提升，因此该方法目前只能限制在室内大小的场景下应用。

&emsp;&emsp;如文献[8]中Strasdat等人提出的方法一样，本文所提出的ORB-SLAM2方法基于深度信息合成立体坐标系，进而提取图像特征。通过这种方法，本文所设计的系统能够处理双目相机或RGB-D相机的输入数据。与以上所有方法不同的是，ORB-SLAM2的后端是基于光束平差法的，并且能够得到全局一致的稀疏重建结果。因此我们的方法是轻量级的并且能够在标准CPU上运行。本文的目标是能够使系统长时间运行并得到全局一致的定位结果而不是进行有着最多细节的稠密重建。然而，由高度精确的关键帧位姿和深度图融合，本文方法依然可以实时得到局部区域的准确三维重建结果，或在全局BA结束后处理所有关键帧的深度图从而得到整个场景的准确三维模型。

# III. ORB-SLAM2

&emsp;&emsp;ORB-SLAM2的双目相机模式和RGB-D相机模式是由基于特征点的ORB-SLAM发展而来，其主要的组成部分如下：

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig2.jpg" width=400px>
    <center>
    <figcaption>图2. ORB-SLAM2由三个主要的并行线程组成：跟踪线程、建图线程和回环检测线程，回环检测能触发第四个线程即全局BA优化。跟踪线程预处理双目或RGB-D相机的输入数据使得系统其余的部分能够不依赖传感器独立运行。ORB-SLAM2还能够基于单目相机的输入数据运行（图中未给出）。(a)系统各线程与组成模块。(b)输入数据预处理。
    </figcaption>
    </center>
</figure>

&emsp;&emsp;系统总体架构如图2所示。ORB-SLAM2包含三个主要的并行线程：

1. 跟踪线程（Tracking）：通过与局部地图进行特征匹配和用运动BA最小化重投影误差来定位每一帧中相机的位置。
2. 局部建图线程（Local Mapping）：管理局部地图并基于局部BA对其进行优化。
3. 回环检测线程（loop closing）：检测大回环并通过位姿图优化修正累积漂移误差。在位姿图优化结束后会触发第四个线程，即全局BA，来计算最优的结构和运动策略。

&emsp;&emsp;本文所设计的系统嵌入了一个基于DBoW2库的位置识别模块来实现重定位功能，避免跟踪线程失效（如出现遮挡）。同时，该模块还可以用于在已建图场景中重新初始化系统和回环检测。ORB-SLAM2系统能够生成关联可见的图用以连接任意两个有共视地图点的关键帧以及连接所有关键帧的最小生成树。这些图结构使得系统能够重新检索关键帧局部窗口，其目的是使跟踪线程和局部建图线程在局部运行，保证系统在大规模环境下工作。当出现回环时，这样的图结构还可以用于位姿图优化。

&emsp;&emsp;ORB-SLAM2系统使用ORB特征完成跟踪、建图和位置识别。ORB特征点具有旋转尺度不变性并且对相机的自动增益和自动曝光以及光照变化具有良好的不变性。除此之外，ORB特征点能够保证实时更快地提取和匹配并在基于Bow的位置识别过程中表现出良好的P/R性能。

&emsp;&emsp;在本节的剩余部分将给出双目或深度信息是如何被应用于系统中的以及系统的哪些部分因此受到影。读者可以在文献[1]即单目ORB-SLAM中获得关于系统每个模块的详细描述。

## A. 单目，近双目和远双目关键点

&emsp;&emsp;如图2(b)所示，ORB-SLAM2作为一种基于特征提取的方法，能够对输入数据进行预处理，在重要的关键点位置提取特征点。预处理结束后，输入的图像信息不再被应用，系统内所有的操作都基于提取的特征点展开，这样保证了系统运行不受传感器是双目相机还是RGB-D相机的限制。本文所设计的系统能够处理单目和双目关键点，这些关键点可以进一步划分为近点和远点。

&emsp;&emsp; 双目关键点由$$\mathbf{x}_\mathbf{s}=(u_L, v_L, u_R)$$三维坐标定义, $$(u_L, v_L)$$为左图的像素坐标，$$u_R$$是右图中的水平坐标。对于双目相机，首先对左右目的两幅图像都进行ORB特征提取，并对每一个左图中的ORB特征点在右图中寻找其匹配点。如果双目相机的图像被矫正过，确保基线水平，那么上述过程可以被高效完成。利用左图ORB特征点的像素坐标和右图对应匹配点的水平坐标可以生成双目关键点，并基于块矫正对这些关键点进行亚像素精细化。对于RGB-D相机，可以在RGB图像中提取ORB特征，并基于文献[8]中Strasdata等人提出的方法，利用每一个特征点的像素坐标$$(u_L, v_L)$$，将其深度值 *d* 转化为右图水平坐标。

$$u_R = u_L - \frac{f_xb}{d}\ \tag{1}$$

其中，$$f_x$$ 是水平焦距，*b* 是结构光发射机和红外相机之间的基线长度，本文所使用的Kinect以及Asus Xtion相机基线长度约为8cm。深度传感器的不确定性由虚拟的右图坐标的不确定性表示。这样一来，系统其它的部分能够对从双目和RGB-D相机输入的图像中提取出的特征点作出同样的处理。

&emsp;&emsp;基于文献[5]中给出的建议，本文规定，当关键点的深度值小于双目或RGB-D相机基线长度的40倍时，将此其定义为近点，否则定义为远点。如果深度值能够被准确地估计，那么近关键点可以安全地从一帧中三角化并提供尺度、平移和旋转信息。另一方面，远点能够提供准确的旋转信息，但是尺度和平移信息则较弱。当远点被从多个视角捕捉时，可以对其进行三角化。

&emsp;&emsp;单目关键点由左图中的$$\mathbf{x_m}=(u_L, v_L)$$定义，并与所有无法进行双目匹配或在使用RGB-D相机的情况下有无效深度值的ORB特征点相关联。这些特征点只能从多个视角进行三角化，它们虽然不能提供尺度信息，但是却有利于旋转和平移的估计结果。

## B. 系统引导

&emsp;&emsp;使用双目和RGB-D相机的好处之一是能够从仅一帧中获得深度信息，而不像使用单目相机时需要做特定的SFM初始化。系统启动阶段会利用首帧图像生成关键帧，将其位姿设置为原点，并基于所有的双目关键点生成初始地图。

## C. 带有单双目约束的光束平差法

&emsp;&emsp;本文提出的系统能够基于BA法在跟踪线程中优化相机位姿（纯运动BA），在局部建图线程中优化局部窗口内的关键帧和地图点（局部BA）以及在回环检测后优化全部的关键帧和地图点（全局BA）。本文基于g2o库中集成的列文伯格-马夸尔特方法求解BA。

&emsp;&emsp;**纯运动BA**优化相机旋转矩阵$$\mathbf{R}\in SO(3)$$和位置$$\mathbf{t}\in\mathbb{R}^3$$，最小化世界坐标系下互相匹配的3-D点云$$\mathbf{X}^i$$与关键点$$\mathbf{x}^i_{(\centerdot)}$$之间的投影误差。其中，关键点可以是单目点$$\mathbf{x}^i_m\in\mathbb{R}^2$$或双目点$$\mathbf{x}^i_s\in\mathbb{R}^3$$，$$i\in\chi$$，$$\chi$$为全部匹配的集合。

$$\{\mathbf{R}, \mathbf{t}\} =\underset{\mathbf{R,t}}{\mathbf{argmin}} \sum_{i\in\chi}\rho(\big\| \mathbf{x}^i_{(\centerdot)} - \pi_{(\centerdot)}(\mathbf{RX}^i+\mathbf{t}) \big\|^2_\Sigma)\tag{2}$$

其中，$$\rho$$是鲁棒Huber代价函数，$$\Sigma$$是与关键点规模相关的协方差矩阵。投影函数$$\pi_{(\centerdot)}$$在使用单目相机时为$$\pi_m$$，使用校正后的双目相机时为$$\pi_s$$，二者定义如下：

$$ \pi_m\Bigg( 
    \left[ 
        \begin{matrix}
        X\\
        Y\\
        Z
        \end{matrix}
    \right] \Bigg) 
    =
    \left[ 
        \begin{matrix}
         f_x\frac{X}{Z}+c_x \\
         f_y\frac{Y}{Z}+c_y   
        \end{matrix}
    \right]
    ,
    \pi_s\Bigg( 
    \left[ 
        \begin{matrix}
        X\\
        Y\\
        Z
        \end{matrix}
    \right] \Bigg) 
    =
    \left[ 
        \begin{matrix}
         f_x\frac{X}{Z}+c_x \\
         f_y\frac{Y}{Z}+c_y \\
         f_x\frac{X-b}{Z}+c_x
        \end{matrix}
    \right] 
    \tag{3}
$$

其中，$$(f_x,f_y)$$ 是焦距，$$(c_x,c_y)$$ 是主点，$$b$$ 是基线，这些参数均由标定获得。


&emsp;&emsp;**局部BA**对一系列共视的关键帧$$\mathcal{K}_L$$和在这些关键帧中观测到的全部点$$\mathcal{P}_L$$进行优化。所有能够观测到$$\mathcal{P}_L$$中的点但是不在$$\mathcal{K}_L$$中的关键帧$$\mathcal{K}_F$$同样有助于构建代价函数，但是在优化中保持不变。将$$\mathcal{X}_k$$定义为$$\mathcal{P}_L$$中的点和关键帧$$k$$中的关键点之间能够相互匹配的点的集合，则优化问题如下：

$$
    \begin{aligned}
        \{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l|i\in\mathcal{P}_L,l\in\mathcal{K}_L\}&=\underset{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l}{\mathbf{argmin}}\sum_{k\in\mathcal{K}_L\cup\mathcal{K}_F}\sum_{j\in\mathcal{X}_k}\rho(E(k,j)) \\

        E(k,j) &= \Big\| \mathbf{x}^j_{(\centerdot)}-\pi_{(\centerdot)}(\mathbf{R}_k\mathbf{X}^j+\mathbf{t}_k)\Big\|^2_\Sigma 
    \end{aligned}
    \tag{4}
$$

&emsp;&emsp;**全局BA**是局部BA的特殊情况，所有地图中的关键帧和点都会被优化，除了原点关键帧始终固定不变以消除测量自由度。

## D. 回环检测与全局BA

&emsp;&emsp;回环检测分两步完成。首先，检测到一个回环并对其进行验证。接着，通过优化位姿图来校正该回环。与存在大尺度漂移的单目ORB-SLAM不同，双目或深度信息使得尺度信息可以观测到，几何校验和位姿图优化不再需要处理尺度漂移，并且是基于刚体变换而不是基于相似性的。

&emsp;&emsp;ORB-SLAM2在位姿图优化后加入了全局BA优化来得到最优解。这一优化过程的代价可能会非常高，因此将会在一个单独的线程中运行，保证系统能够持续建图和检测回环。然而，这会导致全局BA优化输出和当前地图融合的问题。如果优化运行的过程中检测到了一个新的回环，系统能够停止优化并继续回环对齐，这种方式能够再次触发全局BA。当全局BA完成后，系统需要将更新后的关键帧子集和由全局BA优化过的点与未更新的关键帧和优化过程中插入的地图点合并。这是通过生成树将更新的关键帧的校正（如：从未优化位姿到优化后的位姿的转化）传播到非更新的关键帧来实现的。未更新的点根据应用于其参考关键帧的校正进行变换。

## E. 关键帧的插入

&emsp;&emsp;ORB-SLAM2沿用了单目ORB-SLAM中经常插入关键帧并在之后剔除冗余关键帧的策略，远近双目点之间的区别能够引入一个新的关键帧插入条件，这对于一些具有挑战性的环境是至关重要的，因为这些环境中的很大一部分场景都远离双目传感器，如图3所示。在这样的环境中，系统需要有足够多的近点来估计平移，如果跟踪近点的数量下降到$$\tau_t$$以下，并且一帧能够生成至少$$\tau_c$$个新的近双目点，那么系统将插入一个新的关键帧。本文根据经验发现 ，当规定$$\tau_t=100$$，$$\tau_c=70$$时实验效果较好。

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig3.jpg" width=800px>
    <center>
    <figcaption>图3. KITTI 01数据段中的跟踪点。绿色点的深度值小于双目基线长度的40倍，而蓝色点的深度值则远远大于该数据。在这一类数据段中，时常插入足够多的关键帧以使得近点数量足以用于准确的位移估计是至关重要的。远点虽有助于估计旋转，但是只能提供较弱的位移和尺度信息。</figcaption>
    </center>
</figure>

## F. 定位模式

&emsp;&emsp;ORB-SLAM2中加入了一个定位模式，该模式能够在地图已知的场景中有效实现轻量级的长时间定位，前提是环境没有发生巨大的变化。在定位模式下，局部建图和回环检测线程都会挂起，并且可以在需要时由跟踪线程使用重定位方法来持续定位相机。这一模式中，跟踪线程会利用视觉里程计匹配和与地图点的匹配。其中，视觉里程计匹配是指当前帧中的ORB特征点与由双目或深度信息生成的3-D点之间的匹配。这些匹配使得定位模式在未建图区域中仍然有较强的鲁棒性，但是同时会累积漂移误差。地图点匹配能够确保在已建立的地图下实现无漂移误差定位。该模式在本文所附视频中进行了演示。

# IV. 评估

&emsp;&emsp;本文基于三个主流的数据集对ORB-SLAM2的性能进行评估，并与其他先进的SLAM系统进行了对比，所有比较均基于原作者发表的结果和文献中的标准评估指标。本文在CPU为Intel Core i7-4790，内存16GB的台式机上运行ORB-SLAM2。考虑到多线程系统的不确定性，我们对每个数据段运行五次，并取中位数作为评估轨迹的精度。本文开源系统实现方法，包括标定和在全部数据集中运行ORB-SLAM2的步骤介绍。

## A. KITTI 数据集

&emsp;&emsp;文献[2]中的KITTI数据集包含由汽车在城市和高速公路场景中采集的双目数据段。双目传感器基线长度小于54cm，工作频率10Hz，校正后的分辨率为1240x376像素。00,02,05,06,07,09数据段包含回环。ORB-SLAM2能够检测除了09数据段外的所有回环，并且可以实现地图复用。09数据段的回环仅发生在视频流结尾的极少数帧中。表1给出了ORB-SLAM2与目前主流的双目LSD-SLAM在11个不同数据段（有真值）下的测试结果对比。这里要特别说明的是，LSD-SLAM是
已知的唯一给出所有数据段详细测试结果的双目SLAM系统。本文基于两种不同的度量方式来评估误差，一个是文献[3]中提出的绝对平移均方根误差（RMSE）$$t_{abs}$$，另一个是文献[2]中提出的平均相对平移$$t_{rel}$$和旋转$$r_{rel}$$误差。本文提出的ORB-SLAM2在大多数数据集下的测试结果优于LSD-SLAM，并且相对误差普遍小于1%。如图3中所示，01数据段是训练集中唯一一个高速公路场景，并且位移误差略差。在这个数据段中，平移误差更难估计，这是因为高速行驶和低帧率导致只有少数的近点能被跟踪到。然而，由于很多远点能够被长时间跟踪到，旋转可以被准确估计，误差仅有每百米$$0.21^\circ$$。下图4中给出了部分估计轨迹的示例。


<p align="center">
<font size=2>
表1 基于KITTI数据集的精度对比
</font>
</p>


<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/tab1.png" width=600px>
</figure>

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig4.png" width=800px>
    <center>
    <figcaption>图4. KTIII 00, 01, 05和07数据段估计轨迹（黑）与真实轨迹（红）。
    </figcaption>
    </center>
</figure>

&emsp;&emsp;与文献[1]中给出的单目ORB-SLAM的测试结果相比，本文提出的双目版本能够在单目系统失效的01数据段下持续运行。如图3所示，在这一高速公路场景下，仅有少数几帧能看到近点。双目ORB-SLAM2能够从一帧中生成地图点而不需要像单目版本一样通过延迟初始化来找到两个关键帧之间的匹配，这对于该数据段不丢失跟踪是至关重要的。此外，双目系ORB-SLAM2使用公制尺度估计地图和轨迹，并且不会受到尺度漂移的影响，如图5所示。

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig5.png" width=800px>
    <center>
    <figcaption>图5. 基于KITTI数据集08数据段的估计轨迹（黑色）和实际轨迹（红色）。左图为单目ORB-SLAM的结果对比，右图为双目ORB-SLAM2的对比结果。可以看出，在改数据段的测试下，单目ORB-SLAM中存在严重的尺度漂移，尤其是在转弯处。而本文提出的双目ORB-SLAM2则能够准确估计轨迹和地图，无尺度漂移误差。
    </figcaption>
    </center>
</figure>

## B. EuRoC数据集

&emsp;&emsp;最近发布的EURoC数据集包含11个由微型航空器（MAV）在两个不同的大房间和一个大规模工业场景中记录的双目数据段。双目传感器基线小于11cm，以20Hz的频率输出WVGA分辨率的图像。数据段依据MAV的移动速度，场景光照和纹理分为简单，中等和困难几类。在所有的数据段中，MAV都会重新进入场景中，这样使得ORB-SLAM2可以在必要时进行地图复用和回环检测。表2中给出了ORB-SLAM2在全部数据段测试下的绝对平移RMSE结果，并与文献[11]中给出的双目LSD-SLAM测试结果进行了对比。ORB-SLAM2能够实现厘米级的精确定位并且比LSD-SLAM更加准确。由于严重的运动扰动，ORB-SLAM2的跟踪线程会在*V2_03_difficult*数据段发生丢失现象。文献[22]提出，可以通过加入IMU信息解决上述问题。图6中给出了与真实轨迹对比的估计轨迹。


<p align="center">
<font size=2>
表2 基于EuRoC数据集的位移RMSE误差(m)对比
</font>
</p>


<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/tba2.png" width=600px>

</figure>

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig6.png" width=800px>
    <center>
    <figcaption>图6. EuRoC V1_02, V2_02, MH_03和MH_05数据段估计轨迹（黑）与真实轨迹（红）。</figcaption>
    </center>
</figure>

## C. TUM RGB-D数据集

&emsp;&emsp;TUM RGB-D数据集包含由RGB-D传感器采集的室内数据段，这些数据段被分为几类用以评估在不同纹理、光照和结构条件下的目标重建以及SLAM或里程计效果。本文给出了在TUM的一些子序列下的测试结果（使用的数据段通常用于评估RGB-D方法）。表3呈现了ORB-SLAM2与ElasticFusion，Kintinuous，DVO-SLAM以及RGB-D SLAM这些主流SLAM方法在精度方面的对比结果。ORB-SLAM2是唯一一个基于光束平差法（BA）实现后端优化的SLAM方法且在大部分数据段下的测试结果都优于其他方法。文献[1]中给出的RGB-D SLAM测试结果表明，*freigurg2*序列的深度地图存在4%的尺度偏差，这可能是由于标定不准确引起的。本文对这一问题进行了补偿，一定程度上解释了为什么ORB-SLAM2具有更好的测试效果。图7展示了从四个数据段中计算出的关键帧位姿反推传感器深度图所获得的点云图。其中，书桌和海报的良好细节和直线轮廓证明了ORB-SLAM2具有较高的定位精度。


<p align="center">
<font size=2>
表3 TUM RGB-D 数据集
</font>
</p>


<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/tab3.png" width=600px>
</figure>

<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig7-1.png" width=800px>
    <img src="https://effun.xyz/images/ORB-SLAM2/fig7-2.png" width=800px>
    <center>
    <figcaption>图7. 基于估计关键帧位姿和传感器深度图对TUM RGB-D fr3_office, fr1_room, fr2_desk和fr3_nst数据段的稠密点云重建结果</figcaption>
    </center>
</figure>

## D. 计时结果

&emsp;&emsp;为了完善对ORB-SLAM2的评估结果，本文在表4中给出了经过三个具有不同图像分辨率和传感器的数据段测试后得到的计时结果。每个线程消耗的时间和其标准差范围都在表中给出。由于这些数据段只包含一个回环，因此全局BA和回环检测的一部分程序只被执行了一次并且只给出时间结果。在每个数据段中，平均每帧的跟踪时间都低于相机帧率的倒数，这意味着ORB-SLAM2可以实时运行。因为双目图像中的ORB提取是并行的，所以在*V2_02*的双目WVGA图像中提取1000个ORB特征点与在*fr3_office*的单个VGA图像通道中提取相同数量的特征点的时间接近。


<p align="center">
<font size=2>
表4 以毫秒为单位每个线程的耗时结果
</font>
</p>


<figure>
    <img src="https://effun.xyz/images/ORB-SLAM2/tab4.png" width=1000px>

</figure>


&emsp;&emsp;一个回环中的关键帧数量可以视为回环检测线程消耗时间的参考。虽然KITTI 07中的回环包含更多关键帧，但室内*fr3_office*构建的共视图更密集，因此回环融合、位姿图优化和全局BA任务的代价更高。共视图的密度越高，局部地图包含的关键帧和点就越多，局部建图跟踪和局部BA的代价也就越高。

# V. 结论

&emsp;&emsp;本文提出了一种能够用于单目、双目和RGB-D相机的SLAM系统，该系统具有重定位、回环检测和地图复用功能，并且能够实时在标准CPU上运行。本文研究重点为建立可用于在和实验环境一样的大规模场景下长时间可靠定位的全局地图。本文提出的具有重定位功能的定位模式是一种在已知环境下非常鲁棒且无漂移误差的轻量级定位方法。这一模式能够被有效用于实际应用，如VR中，在已有建好的地图的空间下跟踪使用者的视角。

&emsp;&emsp;与目前主流的SLAM方法对比的结果显示，ORB-SLAM2能够在大多数情况下获得最高的精度。在KITTI视觉里程计基准中，ORB-SLAM2是目前最优秀的双目SLAM解决方案。更重要的是，与近来盛行的双目视觉里程计方法相比，ORB-SLAM2能够在已建图场景下获得零漂移误差的定位结果。

&emsp;&emsp;RGB-D模式的测试结果证明了光束平差法相比于直接法和ICP法可以获得最准确的定位效果，并且还有算力低，不需要GPU就能实时运行的优势。

&emsp;&emsp;本文已开源系统代码，并附带有示例和指导，以便于其他研究者可以轻松使用本系统。ORB-SLAM2是已知的第一个开源的视觉SLAM系统，且可以基于单目、双目以及RGB-D相机输入的数据运行。除此以外，本文的源代码包含一个增强现实（AR）的应用案例，该案例基于单目相机完成，展现了ORB-SLAM2的潜力。

&emsp;&emsp;本系统未来的扩展可能包括支持基于无重叠区域多相机、鱼眼相机和广角相机运行以及大规模稠密建图、协同建图和增强运动模糊情况下的鲁棒性。
